{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pypokerengine\n",
        "!pip install tensorflow==2.12"
      ],
      "metadata": {
        "id": "9szRycWq2oNR",
        "outputId": "6976782b-54ef-4f79-d0d2-0cb8a9f2f397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "9szRycWq2oNR",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypokerengine\n",
            "  Downloading PyPokerEngine-1.0.1.tar.gz (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pypokerengine\n",
            "  Building wheel for pypokerengine (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypokerengine: filename=PyPokerEngine-1.0.1-py3-none-any.whl size=34018 sha256=574d001f1dd0165081dbf5566dcf79207246caec384338bd25137b1daa451c4c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/72/6f/588ac6b2ffd3d087573fa78550803417a649f9e992b61d0bc1\n",
            "Successfully built pypokerengine\n",
            "Installing collected packages: pypokerengine\n",
            "Successfully installed pypokerengine-1.0.1\n",
            "Collecting tensorflow==2.12\n",
            "  Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (25.2.10)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.71.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.13.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.5.2)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12)\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (18.1.1)\n",
            "Collecting numpy<1.24,>=1.22 (from tensorflow==2.12)\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.12)\n",
            "  Downloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.17.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12)\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12)\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (4.13.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.5.2,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.5.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.1)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.6.0,>=0.6.0 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting ml_dtypes>=0.5.0 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.3,>=0.5.3 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.5.3-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.35-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.34-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.33-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.31-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (1.15.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.38.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.2.2)\n",
            "Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m116.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl (79.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, protobuf, numpy, keras, gast, jaxlib, google-auth-oauthlib, tensorboard, jax, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.1\n",
            "    Uninstalling jaxlib-0.5.1:\n",
            "      Successfully uninstalled jaxlib-0.5.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.2\n",
            "    Uninstalling google-auth-oauthlib-1.2.2:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.2\n",
            "    Uninstalling jax-0.5.2:\n",
            "      Successfully uninstalled jax-0.5.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "flax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.30 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "orbax-checkpoint 0.11.12 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "bigframes 2.1.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\n",
            "blosc2 3.3.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 jax-0.4.30 jaxlib-0.4.30 keras-2.12.0 numpy-1.23.5 protobuf-4.25.7 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "e7895bfb7c4644b2bb09d6ac2831e348"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import resource\n",
        "import contextlib\n",
        "from collections import deque\n",
        "\n",
        "# Third-party library imports for data handling and machine learning\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models, layers, Input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# PyPokerEngine imports for game setup and player configuration\n",
        "from pypokerengine.api.game import setup_config, start_poker\n",
        "from pypokerengine.players import BasePokerPlayer\n",
        "\n",
        "import json"
      ],
      "metadata": {
        "id": "BUEwGRg-2T5z"
      },
      "id": "BUEwGRg-2T5z",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "NUM_SUITS = 4\n",
        "NUM_RANKS = 13\n",
        "MAX_COMMUNITY_CARDS = 5\n",
        "LARGE_NUMBER = 10000\n",
        "NUM_PLAYERS = 4  # Adjust based on your game setup\n",
        "BETTING_HISTORY_SIZE = 10  # Last 10 actions, adjust as needed\n",
        "\n",
        "default_category = 0\n",
        "\n",
        "\n",
        "def encode_card(card):\n",
        "    \"\"\"Encode a card as a one-hot vector for its rank and suit.\"\"\"\n",
        "    rank, suit = card[1], card[0]\n",
        "    rank_index = '23456789TJQKA'.index(rank)\n",
        "    suit_index = 'SHDC'.index(suit)\n",
        "    rank_one_hot = np.eye(NUM_RANKS)[rank_index]\n",
        "    suit_one_hot = np.eye(NUM_SUITS)[suit_index]\n",
        "    return np.concatenate([rank_one_hot, suit_one_hot])\n",
        "\n",
        "\n",
        "def initialize_network(input_shape, action_size):\n",
        "    # Input layer\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # Shared layers\n",
        "    x = layers.Dense(150, activation='relu')(input_layer)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "\n",
        "    # Action output\n",
        "    action_output = layers.Dense(action_size, activation='linear', name='action_output')(x)\n",
        "\n",
        "    # Bet amount output\n",
        "    bet_amount_output = layers.Dense(5, activation='softmax', name='bet_amount_output')(x)\n",
        "\n",
        "    # Build the model\n",
        "    model = models.Model(inputs=input_layer, outputs=[action_output, bet_amount_output])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                  loss={'action_output': 'mse', 'bet_amount_output': 'categorical_crossentropy'},\n",
        "                  metrics={'action_output': 'accuracy', 'bet_amount_output': 'accuracy'})\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_or_initialize_model(model_path, input_shape, action_size):\n",
        "    print(model_path)\n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"Loading model from {model_path}\")\n",
        "        model = keras.models.load_model(model_path)\n",
        "    else:\n",
        "        print(\"Initializing new model\")\n",
        "        model = initialize_network(input_shape, action_size)\n",
        "    return model\n",
        "\n",
        "\n",
        "def choose_action(state, model, valid_actions, epsilon):\n",
        "    # Initialize default values\n",
        "    best_action = None\n",
        "    bet_amount_category = 0  # Default to the first category\n",
        "\n",
        "    if not valid_actions:\n",
        "        raise ValueError(\"No valid actions provided.\")\n",
        "\n",
        "    if np.random.rand() <= epsilon:\n",
        "        # Exploration: Randomly choose a valid action\n",
        "        best_action = np.random.choice([action['action'] for action in valid_actions])\n",
        "        # Randomly choose a bet amount category (assuming a fixed number of categories)\n",
        "        bet_amount_category = np.random.randint(0, 5)  # Adjust '5' to your number of bet amount categories\n",
        "    else:\n",
        "        # Exploitation: Predict Q-values for actions and bet amounts\n",
        "        predictions = model.predict(state.reshape(1, -1))\n",
        "        q_values = predictions[0][0]  # Assuming the first output corresponds to actions\n",
        "        bet_amounts = predictions[1][0]  # Assuming the second output corresponds to bet amounts\n",
        "\n",
        "        # Map valid actions to their indices\n",
        "        action_indices = {action['action']: idx for idx, action in enumerate(valid_actions)}\n",
        "\n",
        "        # Ensure there is a best action by selecting the first valid action as default\n",
        "        if valid_actions:\n",
        "            best_action = valid_actions[0]['action']\n",
        "\n",
        "        # Choose the best action based on Q-values, considering only valid actions\n",
        "        valid_q_values = [q_values[action_indices[action['action']]] for action in valid_actions]\n",
        "        best_action = valid_actions[np.argmax(valid_q_values)]['action']\n",
        "\n",
        "        # Choose the bet amount category with the highest probability\n",
        "        bet_amount_category = np.argmax(bet_amounts)\n",
        "\n",
        "    if best_action is None:\n",
        "        raise ValueError(\"Unable to determine a best action. Check the logic and inputs.\")\n",
        "\n",
        "    return best_action, bet_amount_category\n",
        "\n",
        "\n",
        "def compute_reward(round_state, action, action_details, is_winner, pot_size_before, pot_size_after, stack_size_before,\n",
        "                   stack_size_after, hole_cards, community_cards):\n",
        "    \"\"\"\n",
        "    Compute a simplified reward considering the action taken and game outcome without estimating hand strength.\n",
        "\n",
        "    Args:\n",
        "        round_state (dict): The state of the current round.\n",
        "        action (str): The action taken by the agent.\n",
        "        action_details (dict): Additional details about the action, such as the bet amount.\n",
        "        is_winner (bool): Indicates whether the player won in this round.\n",
        "        pot_size_before (int): The size of the pot before the action was taken.\n",
        "        pot_size_after (int): The size of the pot after the action was taken.\n",
        "        stack_size_before (int): The player's stack size before the action.\n",
        "        stack_size_after (int): The player's stack size after the action.\n",
        "        hole_cards (list): List of player's hole cards as string.\n",
        "        community_cards (list): List of community cards on the table as string.\n",
        "\n",
        "    Returns:\n",
        "        reward (float): The computed reward.\n",
        "    \"\"\"\n",
        "    reward = 0\n",
        "\n",
        "    # Adjust rewards for winning or losing the round\n",
        "    if is_winner:\n",
        "        reward += 10  # Reward for winning\n",
        "    else:\n",
        "        reward -= 10  # Penalty for losing\n",
        "\n",
        "    # Consider the action taken\n",
        "    if action == 'fold':\n",
        "        reward -= 5  # Discourage folding by default\n",
        "    elif action == 'raise':\n",
        "        bet_amount = action_details.get('amount', 0)\n",
        "        pot_ratio = bet_amount / max(1, pot_size_before)  # Avoid division by zero\n",
        "        reward += 5 * pot_ratio  # Reward for aggressive play when raising\n",
        "\n",
        "    # Adjust reward based on changes in stack size, to encourage preservation of chips\n",
        "    stack_change = stack_size_after - stack_size_before\n",
        "    reward += stack_change / 100.0  # Small reward/penalty based on stack change\n",
        "\n",
        "    return reward\n",
        "\n",
        "\n",
        "def replay(replay_buffer, model, target_model, batch_size, gamma):\n",
        "    \"\"\"Sample a batch of experiences and use them to update the model with dual outputs (action and bet_amount).\n",
        "\n",
        "    Args:\n",
        "        replay_buffer (ReplayBuffer): The replay buffer to sample experiences from.\n",
        "        model (tf.keras.Model): The current model.\n",
        "        target_model (tf.keras.Model): The target model.\n",
        "        batch_size (int): The number of experiences to sample from the buffer.\n",
        "        gamma (float): The discount factor for future rewards.\n",
        "    \"\"\"\n",
        "    # Sample a batch of experiences from the replay buffer\n",
        "    states, actions, bet_amount_categories, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
        "\n",
        "    # Convert lists to numpy arrays for batch processing\n",
        "    states = np.array(states)\n",
        "    next_states = np.array(next_states)\n",
        "    rewards = np.array(rewards)\n",
        "    dones = np.array(dones)\n",
        "\n",
        "    # Predict the Q-values for the next states using the target model\n",
        "    next_q_values_action, next_q_values_bet_amount = target_model.predict(next_states)\n",
        "\n",
        "    # Compute the Q-value targets for actions\n",
        "    targets_action = rewards + gamma * np.max(next_q_values_action, axis=1) * (1 - dones)\n",
        "\n",
        "    # Predict the Q-values for the current states using the model\n",
        "    q_values_action, q_values_bet_amount = model.predict(states)\n",
        "\n",
        "    # Update the Q-values for the actions taken with the computed targets\n",
        "    for i, action in enumerate(actions):\n",
        "        q_values_action[i, action] = targets_action[i]\n",
        "\n",
        "    # Replace None with default_category\n",
        "    bet_amount_categories = np.array([cat if cat is not None else default_category for cat in bet_amount_categories],\n",
        "                                     dtype='int64')\n",
        "\n",
        "    # Convert bet_amount_categories to one-hot encoding for training the bet amount prediction part of the model\n",
        "    bet_amount_categories_one_hot = to_categorical(bet_amount_categories, num_classes=q_values_bet_amount.shape[1])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(states, [q_values_action, bet_amount_categories_one_hot], epochs=1, verbose=0)\n",
        "\n",
        "\n",
        "def update_target_network(main_q_network, target_q_network):\n",
        "    \"\"\"Update the target Q-network's weights with the main Q-network's weights.\n",
        "\n",
        "    Args:\n",
        "        main_q_network (tf.keras.Model): The main Q-network being trained.\n",
        "        target_q_network (tf.keras.Model): The target Q-network used for stability.\n",
        "    \"\"\"\n",
        "    # Get the weights from the main Q-network\n",
        "    main_q_weights = main_q_network.get_weights()\n",
        "\n",
        "    # Set the weights in the target Q-network\n",
        "    target_q_network.set_weights(main_q_weights)\n"
      ],
      "metadata": {
        "id": "1C7HUVrJ3SJG"
      },
      "id": "1C7HUVrJ3SJG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, buffer_size):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "    def add(self, state, action, bet_amount_category, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to the buffer.\"\"\"\n",
        "        # Include bet_amount_category in the stored experience\n",
        "        self.buffer.append((state, action, bet_amount_category, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Sample a batch of experiences from the buffer.\"\"\"\n",
        "        batch = random.sample(self.buffer, min(len(self.buffer), batch_size))\n",
        "        # Unpack bet_amount_category along with other experience components\n",
        "        states, actions, bet_amount_categories, rewards, next_states, dones = zip(*batch)\n",
        "        return states, actions, bet_amount_categories, rewards, next_states, dones\n",
        "\n",
        "\n",
        "class DQNPokerAgent:\n",
        "    def __init__(self, state_size, action_size, model=None, replay_buffer_size=50000, batch_size=32, gamma=0.95,\n",
        "                 epsilon=1, epsilon_min=0.01, epsilon_decay=0.9995):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.replay_buffer = ReplayBuffer(replay_buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.model = model\n",
        "\n",
        "        # If a model is provided, use it; otherwise, initialize a new model\n",
        "        self.q_network = model if model else initialize_network((state_size,), action_size)\n",
        "\n",
        "        # Initialize the target Q-network\n",
        "        self.target_q_network = initialize_network((state_size,), action_size)\n",
        "        self.update_target_network()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"Updates the target Q-network's weights.\"\"\"\n",
        "        update_target_network(self.q_network, self.target_q_network)\n",
        "\n",
        "    def act(self, state, valid_actions):\n",
        "        \"\"\"Choose an action and a bet amount category based on the current state.\"\"\"\n",
        "        action, bet_amount_category = choose_action(np.reshape(state, [1, self.state_size]), self.q_network,\n",
        "                                                    valid_actions, self.epsilon)\n",
        "        return action, bet_amount_category\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Trains the model using a batch of experiences from the replay buffer.\"\"\"\n",
        "        # Sample a batch of experiences from the replay buffer\n",
        "        states, actions, bet_amount_categories, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "        # Proceed with the rest of the training process by updating the Q-network and target Q-network\n",
        "        replay(self.replay_buffer, self.q_network, self.target_q_network, self.batch_size, self.gamma)\n",
        "\n",
        "        # Epsilon decay\n",
        "        print(self.epsilon)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ],
      "metadata": {
        "id": "f9e_Qy-i3ae4"
      },
      "id": "f9e_Qy-i3ae4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNPokerPlayer(BasePokerPlayer):\n",
        "    def __init__(self, dqn_agent, name, state_size, training_mode=True):\n",
        "        super().__init__()\n",
        "        self.dqn_agent = dqn_agent\n",
        "        self.name = name\n",
        "        self.current_state = np.zeros(state_size)\n",
        "        self.prev_state = np.zeros(state_size)\n",
        "        self.hole_card = None\n",
        "        self.last_action = None\n",
        "        self.action_index = None\n",
        "        self.bet_amount_category = None\n",
        "        self.done = False\n",
        "        self.last_action_details = {'action': None, 'amount': 0}  # Initialize with default values\n",
        "\n",
        "        self.training_mode = training_mode  # New flag to indicate training mode\n",
        "        self.games_since_last_learn = 0\n",
        "\n",
        "    def declare_action(self, valid_actions, hole_card, round_state):\n",
        "        if self.current_state is None:\n",
        "            self.current_state = self._extract_state(round_state, hole_card)\n",
        "            print(\"State size:\", round_state.shape)  # Should output (130,)\n",
        "\n",
        "        action, bet_amount_category = self.dqn_agent.act(self.current_state, valid_actions)\n",
        "        self.last_action = action\n",
        "        self.bet_amount_category = bet_amount_category\n",
        "\n",
        "        action_index = self.map_action_to_index(action, valid_actions)\n",
        "        self.action_index = action_index\n",
        "\n",
        "        action_dict = next((item for item in valid_actions if item[\"action\"] == action), None)\n",
        "\n",
        "        if action == 'raise' and isinstance(action_dict['amount'], dict):\n",
        "            min_raise = action_dict['amount']['min']\n",
        "            max_raise = action_dict['amount']['max']\n",
        "            fraction = (self.bet_amount_category + 1) / 5\n",
        "            amount = int(min_raise + fraction * (max_raise - min_raise))\n",
        "        else:\n",
        "            amount = action_dict['amount'] if action_dict else 0\n",
        "        #         print('## amount', amount)\n",
        "\n",
        "        self.last_action_details = {'action': action, 'amount': amount}  # Update action details\n",
        "        #         print(f\"declare_action - last_action_details: {self.last_action_details}\")\n",
        "\n",
        "        return action, amount\n",
        "\n",
        "    def receive_game_start_message(self, game_info):\n",
        "        for seat in game_info['seats']:\n",
        "            if seat['name'] == self.name:\n",
        "                self.uuid = seat['uuid']\n",
        "                break\n",
        "\n",
        "    def receive_round_start_message(self, round_count, hole_card, seats):\n",
        "        self.hole_card = hole_card  # Update hole_card at the start of each round\n",
        "\n",
        "    def receive_game_update_message(self, action, round_state):\n",
        "        self.prev_state = self.current_state  # Store the current state as the previous state\n",
        "\n",
        "        if self.current_state is None:\n",
        "            print('None detected in receive_game_update_message', self.current_state)\n",
        "\n",
        "        if self.hole_card:  # Ensure hole_card has been set\n",
        "            self.current_state = self._extract_state(round_state, self.hole_card)\n",
        "\n",
        "        self.pot_size_before = round_state['pot']['main']['amount']\n",
        "        self.stack_size_before = self._get_stack_size(self.uuid, round_state)\n",
        "\n",
        "    def _get_stack_size(self, player_uuid, round_state):\n",
        "        # Iterate through the list of players (seats) in the round_state\n",
        "        for player_info in round_state['seats']:\n",
        "            # Check if the current player's UUID matches the specified player_uuid\n",
        "            if player_info['uuid'] == player_uuid:\n",
        "                # Return the stack size of the matching player\n",
        "                return player_info['stack']\n",
        "        # Return None or an appropriate default value if the player is not found\n",
        "        return None\n",
        "\n",
        "    def receive_round_result_message(self, winners, hand_info, round_state):\n",
        "        is_winner = any(winner['uuid'] == self.uuid for winner in winners)\n",
        "\n",
        "        # Use the stored action details directly, ensuring it's not None\n",
        "        action_details = self.last_action_details if self.last_action_details is not None else {'action': 'default',\n",
        "                                                                                                'amount': 0}\n",
        "\n",
        "        # Adjust to handle cards that might already be strings\n",
        "        hole_cards = [card if isinstance(card, str) else card.to_str() for card in self.hole_card]\n",
        "        community_cards = [card if isinstance(card, str) else card.to_str() for card in round_state['community_card']]\n",
        "\n",
        "        pot_size_before = self.pot_size_before\n",
        "        pot_size_after = round_state['pot']['main']['amount']\n",
        "        stack_size_before = self.stack_size_before\n",
        "        stack_size_after = self._get_stack_size(self.uuid, round_state)\n",
        "\n",
        "        self.done = True\n",
        "\n",
        "        if self.bet_amount_category is None:\n",
        "            self.bet_amount_category = 0  # Assuming '0' as default category\n",
        "\n",
        "        if self.training_mode:\n",
        "\n",
        "            # Compute the reward with additional parameters\n",
        "            reward = compute_reward(\n",
        "                round_state,\n",
        "                self.last_action,\n",
        "                action_details,\n",
        "                is_winner,\n",
        "                pot_size_before,\n",
        "                pot_size_after,\n",
        "                stack_size_before,\n",
        "                stack_size_after,\n",
        "                hole_cards,\n",
        "                community_cards,\n",
        "                # Number of players\n",
        "            )\n",
        "\n",
        "            self.dqn_agent.replay_buffer.add(self.prev_state, self.action_index, self.bet_amount_category, reward,\n",
        "                                             self.current_state, self.done)\n",
        "            self.games_since_last_learn += 1\n",
        "\n",
        "            if self.games_since_last_learn >= 10:\n",
        "                self.dqn_agent.learn()\n",
        "                self.games_since_last_learn = 0\n",
        "\n",
        "        # Reset for the next round\n",
        "        self.done = False\n",
        "        self.prev_state = None\n",
        "        self.action_index = None\n",
        "        self.bet_amount_category = None\n",
        "        self.last_action_details = {'action': None, 'amount': 0}\n",
        "\n",
        "    def _get_stack_size(self, uuid, round_state):\n",
        "        for player in round_state['seats']:\n",
        "            if player['uuid'] == uuid:\n",
        "                return player['stack']\n",
        "        return None\n",
        "\n",
        "    def _extract_state(self, round_state, hole_card):\n",
        "        MAX_COMMUNITY_CARDS = 5\n",
        "        MAX_PLAYERS = 10  # Define the maximum number of players you expect in any game\n",
        "        LARGE_NUMBER = 10000  # Normalization factor for large values like stacks and pots\n",
        "\n",
        "        # Encoding hole and community cards\n",
        "        hole_cards_encoded = [encode_card(card) for card in hole_card]\n",
        "        community_cards_encoded = [encode_card(card) for card in round_state['community_card']]\n",
        "\n",
        "        # Determine the vector size for a single card\n",
        "        N = len(\n",
        "            encode_card(hole_card[0])) if hole_card else 26  # Example fallback size if encode_card size is not standard\n",
        "\n",
        "        # Flatten encoded cards into single vectors\n",
        "        hole_cards_vector = np.concatenate(hole_cards_encoded) if hole_cards_encoded else np.zeros(N * 2)\n",
        "        community_cards_vector = np.concatenate(community_cards_encoded) if community_cards_encoded else np.zeros(\n",
        "            N * len(community_cards_encoded))\n",
        "\n",
        "        # Define a zero vector for padding community cards\n",
        "        zero_vector = np.zeros(N)\n",
        "        padded_community_cards = np.concatenate(\n",
        "            [community_cards_vector] + [zero_vector for _ in range(MAX_COMMUNITY_CARDS - len(community_cards_encoded))])\n",
        "\n",
        "        # Additional game features\n",
        "        pot_size = np.array([round_state['pot']['main']['amount'] / LARGE_NUMBER])  # Normalize pot size\n",
        "\n",
        "        # Encoding player stack sizes and positions\n",
        "        seats = round_state['seats']\n",
        "        stack_sizes = [seat['stack'] / LARGE_NUMBER for seat in seats if 'stack' in seat]\n",
        "        positions = [1 if i == round_state.get('dealer_btn') else 0 for i in range(len(seats))]\n",
        "\n",
        "        # Optional: Number of active players (can be useful in strategy decision making)\n",
        "        active_players = [1 if 'folded' not in seat['state'] else 0 for seat in seats]\n",
        "\n",
        "        # Padding for player-related features if there are fewer players than MAX_PLAYERS\n",
        "        stack_sizes += [0] * (MAX_PLAYERS - len(stack_sizes))\n",
        "        positions += [0] * (MAX_PLAYERS - len(positions))\n",
        "        active_players += [0] * (MAX_PLAYERS - len(active_players))\n",
        "\n",
        "        # Convert lists to numpy arrays\n",
        "        stack_sizes = np.array(stack_sizes)\n",
        "        positions = np.array(positions)\n",
        "        active_players = np.array(active_players)\n",
        "\n",
        "        # Concatenate all parts to form the full state vector\n",
        "        state = np.concatenate(\n",
        "            [hole_cards_vector, padded_community_cards, pot_size, stack_sizes, positions, active_players])\n",
        "\n",
        "        return state\n",
        "\n",
        "    def map_action_to_index(self, action, valid_actions):\n",
        "        # Convert the action to an index. This implementation depends on how your actions are structured.\n",
        "        action_dict = {act['action']: idx for idx, act in enumerate(valid_actions)}\n",
        "        return action_dict.get(action, -1)  # Return -1 or another value for invalid actions\n",
        "\n",
        "    def receive_street_start_message(self, street, round_state):\n",
        "        pass\n",
        "\n",
        "\n",
        "def setup_poker_game(dqn_agent1, initial_stack=1000):\n",
        "    state_size = 150\n",
        "    action_size = 3\n",
        "\n",
        "    # Randomly decide the number of players (between 2 and 10 for example)\n",
        "    num_players = random.randint(2, 5)\n",
        "\n",
        "    config = setup_config(max_round=100, initial_stack=initial_stack, small_blind_amount=10)\n",
        "\n",
        "    # Register the main training player\n",
        "    dqn_player1 = DQNPokerPlayer(dqn_agent1, \"DQN_Player_Train\", state_size, training_mode=True)\n",
        "    config.register_player(name=\"DQN_Player_Train\", algorithm=dqn_player1)\n",
        "\n",
        "    # Optionally, load or initialize additional models if needed\n",
        "    # model_path = './my_dqn_model.keras'\n",
        "\n",
        "    # Register other players, potentially older versions or different strategies\n",
        "    for i in range(1, num_players):\n",
        "        dqn_model = load_or_initialize_model(model_path, state_size, action_size)\n",
        "        dqn_agent = DQNPokerAgent(state_size, action_size, model=dqn_model, epsilon=0.0)\n",
        "        dqn_player = DQNPokerPlayer(dqn_agent, f\"DQN_Player_Old_{i}\", state_size, training_mode=False)\n",
        "        config.register_player(name=f\"DQN_Player_Old_{i}\", algorithm=dqn_player)\n",
        "\n",
        "    # # Optionally, add other types of players\n",
        "    # config.register_player(name=\"Random_Player\", algorithm=RandomPlayer())\n",
        "    # config.register_player(name=\"SmartPlayer\", algorithm=SmartPlayer())\n",
        "\n",
        "    return config"
      ],
      "metadata": {
        "id": "G6wxkz9j3uIU"
      },
      "id": "G6wxkz9j3uIU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is to suppress stdouts of tensorflow which I can't somhow suppress\n",
        "@contextlib.contextmanager\n",
        "def suppress_stdout():\n",
        "    original_stdout = sys.stdout\n",
        "    sys.stdout = open(os.devnull, 'w')\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        sys.stdout.close()\n",
        "        sys.stdout = original_stdout\n",
        "\n",
        "\n",
        "from time import gmtime, strftime\n",
        "\n",
        "\n",
        "# Set maximum heap size (e.g., 500 MB)\n",
        "max_heap_size = 16 * 1024 * 1024 * 1024  # 16 GB in bytes\n",
        "\n",
        "# Set memory limit\n",
        "## resource.setrlimit(resource.RLIMIT_AS, (max_heap_size, max_heap_size))\n",
        "\n",
        "# Initialize dictionaries to store metrics for each player\n",
        "total_gains = {}\n",
        "cumulative_rewards = {}\n",
        "win_rates = {}\n",
        "\n",
        "# Define model path\n",
        "model_name = 'my_dqn'\n",
        "model_path = './' + model_name + '_model.keras'\n",
        "weights_path = './' + model_name + '_weights.h5'\n",
        "config_path = './' + model_name + '_config.json'\n",
        "\n",
        "# Define the input shape and action size for your model\n",
        "input_shape = (150,)\n",
        "action_size = 3\n",
        "state_size = input_shape[0]\n",
        "\n",
        "# Initialize or load the main model\n",
        "dqn_model = load_or_initialize_model(model_path, input_shape, action_size)\n",
        "dqn_agent = DQNPokerAgent(state_size, action_size, model=dqn_model, epsilon=1)\n",
        "# Save the current model for the other players\n",
        "dqn_model.save(model_path)\n",
        "\n",
        "\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(dqn_model.get_config(), f, ensure_ascii=False, indent=4)\n",
        "\n",
        "# cfg = dqn_model.to_json()\n",
        "# with open(model_path + '.json', 'w') as f:\n",
        "#     f.write(cfg)\n",
        "\n",
        "\n",
        "# Training loop...\n",
        "for episode in range(1000000):\n",
        "\n",
        "    with suppress_stdout():\n",
        "\n",
        "        initial_stack = random.randint(1,\n",
        "                                       10) * 100  # every game will have different initial stacks for now small blind remains 10\n",
        "\n",
        "        #     if 1==1:\n",
        "        # Setup and start the poker game with the current model playing against its previous version\n",
        "        config = setup_poker_game(dqn_agent, initial_stack=initial_stack)\n",
        "        game_result = start_poker(config, verbose=0)\n",
        "\n",
        "        # Update metrics based on game results...\n",
        "        for player in game_result['players']:\n",
        "            name = player['name']\n",
        "            stack_change = player['stack'] - initial_stack\n",
        "\n",
        "            # Initialize player metrics if new\n",
        "            if name not in total_gains:\n",
        "                total_gains[name] = [0]\n",
        "                cumulative_rewards[name] = []\n",
        "                win_rates[name] = []\n",
        "\n",
        "            # Update player metrics\n",
        "            total_gains[name].append(total_gains[name][-1] + stack_change)\n",
        "            cumulative_rewards[name].append(total_gains[name][-1])\n",
        "            win_rate = sum(r > 0 for r in total_gains[name]) / len(total_gains[name])\n",
        "            win_rates[name].append(win_rate)\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
        "        print(f\" Epsilon after episode {episode}: {dqn_agent.epsilon}\")\n",
        "\n",
        "        # Save the current model\n",
        "        dqn_model.save(model_path)\n",
        "        #         print(\"Saved the current model.\")\n",
        "\n",
        "        # Clear the current figure to ensure old plots are not shown\n",
        "        plt.clf()\n",
        "\n",
        "        # Plot metrics\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Plot cumulative rewards\n",
        "        plt.subplot(1, 2, 1)\n",
        "        for name, rewards in cumulative_rewards.items():\n",
        "            plt.plot(rewards, label=f'{name} Total Gains')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Total Gains')\n",
        "        plt.title('Total Gains per Episode')\n",
        "        plt.legend()\n",
        "\n",
        "        # Plot win rates\n",
        "        plt.subplot(1, 2, 2)\n",
        "        for name, rates in win_rates.items():\n",
        "            plt.plot(rates, label=f'{name} Win Rate')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Win Rate')\n",
        "        plt.title('Win Rate per Episode')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "GksSpUnl3x9p"
      },
      "id": "GksSpUnl3x9p",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}